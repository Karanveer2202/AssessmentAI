**Familiarizing myself with Generative AI. **

First, I understood the GPT-2 paperâ€™s first 2 sections. I familiarized myself with the concepts of transform layers, multi-head self-attention mechanisms, etc. for a better working understanding of GPT-2.
Then I worked on implementing the basic components: 
	1. Token and positional embeddings 
	2. scaled dot-product attention, and extended this to multi-head attention 
  3. feed-forward network
  4. then I created a single transformer block
  5. stacked multiple transformer blocks 
  6. tried loading pre-trained GPT-2 weights but I got stuck here
  7. finally got it to work.
  
References: https://github.com/towardsautonomy/QAGAN 
