# Assessment
This is for the Coding Assessment for ContloAI's coding assessment.
First, I understood the GPT-2 paperâ€™s first 2 sections. I familiarized myself with the concepts of transform layers, multi-head self attention mechanisms etc. for a better working understanding of gpt-2.

Task 1:
Then i worked on implementing the basic components: 
	1. Token and positional embeddings 
	2. scaled dot-product attention, extended this to multi-head attention 
  3. feed-forward network
  4. then i created a single transformer block
  5. stacked multiple transformer blocks 
  6. tried loading pre-trained gpt-2 weights but I got stuck here
  7. finally got it to work.

  Task 2: 
  
References: https://github.com/towardsautonomy/QAGAN 
